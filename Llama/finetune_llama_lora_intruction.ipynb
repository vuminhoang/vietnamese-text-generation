{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6mxqPutADMJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "k9UnYm_hAKXx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Saving model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vB49Q4EAQRe",
        "outputId": "f9d8a5f6-fe1e-40b3-9645-b7b4d72a5e05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "open_instruct_dataset = load_dataset(\"5CD-AI/Vietnamese-Ecommerce-Alpaca\", split=\"train\")"
      ],
      "metadata": {
        "id": "5saSgIJFElJ5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_instruct_dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75eQCbBcFrSQ",
        "outputId": "63f096ac-13ae-4cd8-a71e-2926c71a2b5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'S·ª≠ d·ª•ng t√∫i gi·∫∑t c√≥ th·ªÉ mang l·∫°i m·ªôt s·ªë l·ª£i √≠ch, bao g·ªìm:\\n\\n- B·∫£o v·ªá qu·∫ßn √°o: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p b·∫£o v·ªá qu·∫ßn √°o kh·ªèi b·ªã h∆∞ h·ªèng trong qu√° tr√¨nh gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi qu·∫ßn √°o m·ªèng manh ho·∫∑c d·ªÖ b·ªã r√°ch.\\n\\n- Gi√∫p gi·∫∑t s·∫°ch h∆°n: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p qu·∫ßn √°o ƒë∆∞·ª£c gi·∫∑t s·∫°ch h∆°n b·∫±ng c√°ch t·∫°o ra chuy·ªÉn ƒë·ªông t·ªët h∆°n trong m√°y gi·∫∑t v√† gi√∫p ch·∫•t t·∫©y r·ª≠a ti·∫øp x√∫c v·ªõi qu·∫ßn √°o t·ªët h∆°n.\\n\\n- Gi·∫£m xo·∫Øn r·ªëi: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m xo·∫Øn r·ªëi qu·∫ßn √°o trong qu√° tr√¨nh gi·∫∑t, gi√∫p qu·∫ßn √°o d·ªÖ ·ªßi h∆°n v√† tr√¥ng ƒë·∫πp h∆°n.\\n\\n- Ti·∫øt ki·ªám th·ªùi gian: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p ti·∫øt ki·ªám th·ªùi gian b·∫±ng c√°ch gi·∫£m th·ªùi gian ·ªßi qu·∫ßn √°o v√† gi√∫p qu·∫ßn √°o kh√¥ nhanh h∆°n.\\n\\n- Gi·∫£m ti·∫øng ·ªìn: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m ti·∫øng ·ªìn t·ª´ m√°y gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi m√°y gi·∫∑t c≈© ho·∫∑c b·ªã h·ªèng.',\n",
              " 'instruction': 'N√≥i cho t√¥i bi·∫øt t·∫°i sao n√™n s·ª≠ d·ª•ng t√∫i gi·∫∑t.',\n",
              " 'input': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 5020\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1yjf3eiG-Hs",
        "outputId": "8cd8c826-a4b7-479d-e07e-cfdafe3ab8a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "BdxQWH-nG-tA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_row_as_instruction_prompt(example):\n",
        "    has_input = example.get('input', None) is not None\n",
        "\n",
        "    if has_input:\n",
        "        primer_prompt = (\"Below is an instruction that describes a task, paired with an input \"\n",
        "                         \"that provides further context. Write a response that appropriately completes the request.\")\n",
        "        input_template = f\"### Input: \\n{example['input']}\\n\\n\"\n",
        "    else:\n",
        "        primer_prompt = (\"Below is an instruction that describes a task. \"\n",
        "                         \"Write a response that appropriately completes the request.\")\n",
        "        input_template = \"\"\n",
        "\n",
        "    instruction_template = f\"### Instruction: \\n{example['instruction']}\\n\\n\"\n",
        "\n",
        "    if example.get('output', None):\n",
        "        response_template = f\"### Response: \\n{example['output']}\\n\\n\"\n",
        "    else:\n",
        "        response_template = \"\"\n",
        "\n",
        "    # Wrap the resulting string in a list\n",
        "    return f\"{primer_prompt}\\n\\n{instruction_template}{input_template}{response_template}\"\n",
        "\n",
        "\n",
        "def formatting_prompt(examples):\n",
        "    \"\"\"\n",
        "    Formats a batch of examples using the format_row_as_instruction_prompt logic.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary with keys \"instruction\", \"input\", and \"output\".\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing a list of formatted text prompts.\n",
        "    \"\"\"\n",
        "    formatted_texts = []\n",
        "\n",
        "    for instruction, input_, output in zip(\n",
        "        examples.get(\"instruction\", []),\n",
        "        examples.get(\"input\", []),\n",
        "        examples.get(\"output\", []),\n",
        "    ):\n",
        "        # Create a single example dictionary\n",
        "        example = {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_,\n",
        "            \"output\": output\n",
        "        }\n",
        "\n",
        "        # Use format_row_as_instruction_prompt to format the example\n",
        "        formatted_text = format_row_as_instruction_prompt(example) + EOS_TOKEN\n",
        "        formatted_texts.append(formatted_text)\n",
        "\n",
        "    return {\"text\": formatted_texts}"
      ],
      "metadata": {
        "id": "Zqm2j-rGFyyY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = open_instruct_dataset.map(formatting_prompt, batched=True)"
      ],
      "metadata": {
        "id": "gKUz2oR1FLs6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[1][\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gbgJnhJ0GtUX",
        "outputId": "91fd7764-a428-430b-b260-479d1c438866"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction: \\nN√≥i cho t√¥i bi·∫øt t·∫°i sao n√™n s·ª≠ d·ª•ng t√∫i gi·∫∑t.\\n\\n### Input: \\n\\n\\n### Response: \\nS·ª≠ d·ª•ng t√∫i gi·∫∑t c√≥ th·ªÉ mang l·∫°i m·ªôt s·ªë l·ª£i √≠ch, bao g·ªìm:\\n\\n- B·∫£o v·ªá qu·∫ßn √°o: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p b·∫£o v·ªá qu·∫ßn √°o kh·ªèi b·ªã h∆∞ h·ªèng trong qu√° tr√¨nh gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi qu·∫ßn √°o m·ªèng manh ho·∫∑c d·ªÖ b·ªã r√°ch.\\n\\n- Gi√∫p gi·∫∑t s·∫°ch h∆°n: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p qu·∫ßn √°o ƒë∆∞·ª£c gi·∫∑t s·∫°ch h∆°n b·∫±ng c√°ch t·∫°o ra chuy·ªÉn ƒë·ªông t·ªët h∆°n trong m√°y gi·∫∑t v√† gi√∫p ch·∫•t t·∫©y r·ª≠a ti·∫øp x√∫c v·ªõi qu·∫ßn √°o t·ªët h∆°n.\\n\\n- Gi·∫£m xo·∫Øn r·ªëi: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m xo·∫Øn r·ªëi qu·∫ßn √°o trong qu√° tr√¨nh gi·∫∑t, gi√∫p qu·∫ßn √°o d·ªÖ ·ªßi h∆°n v√† tr√¥ng ƒë·∫πp h∆°n.\\n\\n- Ti·∫øt ki·ªám th·ªùi gian: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p ti·∫øt ki·ªám th·ªùi gian b·∫±ng c√°ch gi·∫£m th·ªùi gian ·ªßi qu·∫ßn √°o v√† gi√∫p qu·∫ßn √°o kh√¥ nhanh h∆°n.\\n\\n- Gi·∫£m ti·∫øng ·ªìn: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m ti·∫øng ·ªìn t·ª´ m√°y gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi m√°y gi·∫∑t c≈© ho·∫∑c b·ªã h·ªèng.\\n\\n<|end_of_text|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ootFzQp2MjYX",
        "outputId": "39f047d8-9f88-41ed-889f-f9513959bc87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['output', 'instruction', 'input', 'text'],\n",
              "    num_rows: 69303\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset = training_data.select(range(5000))"
      ],
      "metadata": {
        "id": "kCuJ9MNAM6n5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "KMqYXpANHOS2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 32,\n",
        "    loftq_config = None,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWVlwXA_FVKQ",
        "outputId": "f730b57e-589c-4d05-8ed5-b53672190e2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.12.12 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV8eILyZTz5R",
        "outputId": "317f0bd2-e6aa-4e55-dbff-b058764d53fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['output', 'instruction', 'input', 'text'],\n",
              "    num_rows: 5000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=subset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        fp16 = True,\n",
        "        logging_steps=2,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=1512,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "f5d-rnoaTpay",
        "outputId": "d5802187-4344-465a-a010-2f15febe01b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 288 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 16 | Total steps = 18\n",
            " \"-____-\"     Number of trainable parameters = 11,272,192\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 16:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.854100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.770700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.460200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.411700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.372400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.318600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.294400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.249400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=18, training_loss=1.473856767018636, metrics={'train_runtime': 1050.9247, 'train_samples_per_second': 0.274, 'train_steps_per_second': 0.017, 'total_flos': 8539395643146240.0, 'train_loss': 1.473856767018636, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save."
      ],
      "metadata": {
        "id": "aEkfl2tobQis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ61_p0ZbRNQ",
        "outputId": "4e0bc248-21af-43f9-888b-60b4666e86d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "Getl-MFzd2fT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"N√≥i cho t√¥i bi·∫øt t·∫°i sao n√™n s·ª≠ d·ª•ng m√°y gi·∫∑t\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMEF7zIobZmm",
        "outputId": "26551bfd-f8b4-48cc-fbe1-cc37e4dc90f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "N√≥i cho t√¥i bi·∫øt t·∫°i sao n√™n s·ª≠ d·ª•ng m√°y gi·∫∑t\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "M√°y gi·∫∑t l√† m·ªôt thi·∫øt b·ªã ƒëi·ªán t·ª≠ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ r·ª≠a c√°c v·∫≠t th·ªÉ nh∆∞ qu·∫ßn √°o, ƒë·ªì ƒë·∫°c, v.v. M√°y gi·∫∑t th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c ph√≤ng t·∫Øm, ph√≤ng b·∫øp, ph√≤ng kh√°ch, v.v. M√°y gi·∫∑t c√≥ nhi·ªÅu lo·∫°i kh√°c nhau, t√πy thu·ªôc v√†o nhu c·∫ßu v√† s·ªü th√≠ch c·ªßa ng∆∞·ªùi d√πng. M·ªôt s·ªë t√≠nh nƒÉng n·ªïi b·∫≠t c·ªßa m√°y gi·∫∑t bao g·ªìm: - M√°y gi·∫∑t c√≥ nhi·ªÅu lo·∫°i kh√°c nhau, t√πy thu·ªôc v√†o nhu c·∫ßu v√† s·ªü th√≠ch c·ªßa ng∆∞·ªùi d√πng. - M√°y gi·∫∑t c√≥ nhi·ªÅu lo·∫°i kh√°c nhau, t√πy thu·ªôc v√†o nhu c·∫ßu v√† s·ªü th√≠ch c·ªßa ng∆∞·ªùi d√πng\n"
          ]
        }
      ]
    }
  ]
}