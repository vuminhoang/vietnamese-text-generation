{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r6mxqPutADMJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k9UnYm_hAKXx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vB49Q4EAQRe",
        "outputId": "80aae942-f23a-436a-e6ea-ae4cea35cd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Saving model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5saSgIJFElJ5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "open_instruct_dataset = load_dataset(\"5CD-AI/Vietnamese-Ecommerce-Alpaca\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75eQCbBcFrSQ",
        "outputId": "f852a7ce-6a87-4f5e-9de2-63bfc3ff2e94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'S·ª≠ d·ª•ng t√∫i gi·∫∑t c√≥ th·ªÉ mang l·∫°i m·ªôt s·ªë l·ª£i √≠ch, bao g·ªìm:\\n\\n- B·∫£o v·ªá qu·∫ßn √°o: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p b·∫£o v·ªá qu·∫ßn √°o kh·ªèi b·ªã h∆∞ h·ªèng trong qu√° tr√¨nh gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi qu·∫ßn √°o m·ªèng manh ho·∫∑c d·ªÖ b·ªã r√°ch.\\n\\n- Gi√∫p gi·∫∑t s·∫°ch h∆°n: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p qu·∫ßn √°o ƒë∆∞·ª£c gi·∫∑t s·∫°ch h∆°n b·∫±ng c√°ch t·∫°o ra chuy·ªÉn ƒë·ªông t·ªët h∆°n trong m√°y gi·∫∑t v√† gi√∫p ch·∫•t t·∫©y r·ª≠a ti·∫øp x√∫c v·ªõi qu·∫ßn √°o t·ªët h∆°n.\\n\\n- Gi·∫£m xo·∫Øn r·ªëi: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m xo·∫Øn r·ªëi qu·∫ßn √°o trong qu√° tr√¨nh gi·∫∑t, gi√∫p qu·∫ßn √°o d·ªÖ ·ªßi h∆°n v√† tr√¥ng ƒë·∫πp h∆°n.\\n\\n- Ti·∫øt ki·ªám th·ªùi gian: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p ti·∫øt ki·ªám th·ªùi gian b·∫±ng c√°ch gi·∫£m th·ªùi gian ·ªßi qu·∫ßn √°o v√† gi√∫p qu·∫ßn √°o kh√¥ nhanh h∆°n.\\n\\n- Gi·∫£m ti·∫øng ·ªìn: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m ti·∫øng ·ªìn t·ª´ m√°y gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi m√°y gi·∫∑t c≈© ho·∫∑c b·ªã h·ªèng.',\n",
              " 'instruction': 'N√≥i cho t√¥i bi·∫øt t·∫°i sao n√™n s·ª≠ d·ª•ng t√∫i gi·∫∑t.',\n",
              " 'input': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "open_instruct_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o1yjf3eiG-Hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12829fa-cdd1-488e-fa07-8aac94e9cc58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 1024\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BdxQWH-nG-tA"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zqm2j-rGFyyY"
      },
      "outputs": [],
      "source": [
        "def format_row_as_instruction_prompt(example):\n",
        "    has_input = example.get('input', None) is not None\n",
        "\n",
        "    if has_input:\n",
        "        primer_prompt = (\"Below is an instruction that describes a task, paired with an input \"\n",
        "                         \"that provides further context. Write a response that appropriately completes the request.\")\n",
        "        input_template = f\"### Input: \\n{example['input']}\\n\\n\"\n",
        "    else:\n",
        "        primer_prompt = (\"Below is an instruction that describes a task. \"\n",
        "                         \"Write a response that appropriately completes the request.\")\n",
        "        input_template = \"\"\n",
        "\n",
        "    instruction_template = f\"### Instruction: \\n{example['instruction']}\\n\\n\"\n",
        "\n",
        "    if example.get('output', None):\n",
        "        response_template = f\"### Response: \\n{example['output']}\\n\\n\"\n",
        "    else:\n",
        "        response_template = \"\"\n",
        "\n",
        "    # Wrap the resulting string in a list\n",
        "    return f\"{primer_prompt}\\n\\n{instruction_template}{input_template}{response_template}\"\n",
        "\n",
        "\n",
        "def formatting_prompt(examples):\n",
        "    \"\"\"\n",
        "    Formats a batch of examples using the format_row_as_instruction_prompt logic.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary with keys \"instruction\", \"input\", and \"output\".\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing a list of formatted text prompts.\n",
        "    \"\"\"\n",
        "    formatted_texts = []\n",
        "\n",
        "    for instruction, input_, output in zip(\n",
        "        examples.get(\"instruction\", []),\n",
        "        examples.get(\"input\", []),\n",
        "        examples.get(\"output\", []),\n",
        "    ):\n",
        "        # Create a single example dictionary\n",
        "        example = {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_,\n",
        "            \"output\": output\n",
        "        }\n",
        "\n",
        "        # Use format_row_as_instruction_prompt to format the example\n",
        "        formatted_text = format_row_as_instruction_prompt(example) + EOS_TOKEN\n",
        "        formatted_texts.append(formatted_text)\n",
        "\n",
        "    return {\"text\": formatted_texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gKUz2oR1FLs6"
      },
      "outputs": [],
      "source": [
        "training_data = open_instruct_dataset.map(formatting_prompt, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gbgJnhJ0GtUX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "a437f72b-8b5c-4d60-da73-ab38537f6de6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction: \\nN√≥i cho t√¥i bi·∫øt t·∫°i sao n√™n s·ª≠ d·ª•ng t√∫i gi·∫∑t.\\n\\n### Input: \\n\\n\\n### Response: \\nS·ª≠ d·ª•ng t√∫i gi·∫∑t c√≥ th·ªÉ mang l·∫°i m·ªôt s·ªë l·ª£i √≠ch, bao g·ªìm:\\n\\n- B·∫£o v·ªá qu·∫ßn √°o: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p b·∫£o v·ªá qu·∫ßn √°o kh·ªèi b·ªã h∆∞ h·ªèng trong qu√° tr√¨nh gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi qu·∫ßn √°o m·ªèng manh ho·∫∑c d·ªÖ b·ªã r√°ch.\\n\\n- Gi√∫p gi·∫∑t s·∫°ch h∆°n: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p qu·∫ßn √°o ƒë∆∞·ª£c gi·∫∑t s·∫°ch h∆°n b·∫±ng c√°ch t·∫°o ra chuy·ªÉn ƒë·ªông t·ªët h∆°n trong m√°y gi·∫∑t v√† gi√∫p ch·∫•t t·∫©y r·ª≠a ti·∫øp x√∫c v·ªõi qu·∫ßn √°o t·ªët h∆°n.\\n\\n- Gi·∫£m xo·∫Øn r·ªëi: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m xo·∫Øn r·ªëi qu·∫ßn √°o trong qu√° tr√¨nh gi·∫∑t, gi√∫p qu·∫ßn √°o d·ªÖ ·ªßi h∆°n v√† tr√¥ng ƒë·∫πp h∆°n.\\n\\n- Ti·∫øt ki·ªám th·ªùi gian: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p ti·∫øt ki·ªám th·ªùi gian b·∫±ng c√°ch gi·∫£m th·ªùi gian ·ªßi qu·∫ßn √°o v√† gi√∫p qu·∫ßn √°o kh√¥ nhanh h∆°n.\\n\\n- Gi·∫£m ti·∫øng ·ªìn: T√∫i gi·∫∑t c√≥ th·ªÉ gi√∫p gi·∫£m ti·∫øng ·ªìn t·ª´ m√°y gi·∫∑t, ƒë·∫∑c bi·ªát l√† ƒë·ªëi v·ªõi m√°y gi·∫∑t c≈© ho·∫∑c b·ªã h·ªèng.\\n\\n<|end_of_text|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "training_data[1][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ootFzQp2MjYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c74f1a-6190-4f99-9ee0-2173b2df2eb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['output', 'instruction', 'input', 'text'],\n",
              "    num_rows: 69303\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KMqYXpANHOS2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hWVlwXA_FVKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a3f33b-69b6-45f5-fdc6-5c462e24177e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.12.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 32,\n",
        "    loftq_config = None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OV8eILyZTz5R"
      },
      "outputs": [],
      "source": [
        "train_subset = training_data.select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f5d-rnoaTpay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12ad33f-8c60-44a1-9ecb-085cf6e11482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_subset,  # train set\n",
        "    dataset_text_field=\"text\", # field to use\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=1e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=4, # gi·∫£ l·∫≠p batch_size l√† 16\n",
        "        num_train_epochs=1,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=1512\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9R1Y9gosdm8t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "6f832a92-a42e-43f1-9c76-163b47a790a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 281 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 32 | Total steps = 9\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 12:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.370300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.364000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.381400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.282500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.241300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.114600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.078200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEkfl2tobQis"
      },
      "source": [
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rZ61_p0ZbRNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9987c87-bf22-4a64-911c-8d58592f28a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRhYCjAD8yUd",
        "outputId": "5d7034c9-1f60-47ed-928e-35f84ee0d6f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  2 07:38:09 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0              32W /  70W |  12603MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# restart session and run predict\n",
        "cause out of ram, I tried some methods to prevent it but it's not work!\n",
        "\n",
        "Remember to save your content/lora_model before restart runtime, or you will loss all your process!"
      ],
      "metadata": {
        "id": "iP7tmSzj9jmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aMEF7zIobZmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8ff7e8-3113-40c0-cc19-7c3335cc7822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "T·∫°i sao t√¥i n√™n d√πng m√°y gi·∫∑t?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "M√°y gi·∫∑t gi√∫p b·∫°n ti·∫øt ki·ªám th·ªùi gian v√† c√¥ng s·ª©c. B·∫°n c√≥ th·ªÉ gi·∫∑t qu·∫ßn √°o m·ªôt l·∫ßn v√† kh√¥ng c·∫ßn ph·∫£i gi·∫∑t l·∫°i nhi·ªÅu l·∫ßn. M√°y gi·∫∑t c≈©ng gi√∫p qu·∫ßn √°o s·∫°ch h∆°n v√† m·ªÅm h∆°n. Ngo√†i ra, m√°y gi·∫∑t c≈©ng gi√∫p b·∫°n ti·∫øt ki·ªám ti·ªÅn. B·∫°n c√≥ th·ªÉ gi·∫∑t qu·∫ßn √°o nhi·ªÅu l·∫ßn v√† kh√¥ng c·∫ßn ph·∫£i mua qu·∫ßn √°o m·ªõi.\n",
            "\n",
            "<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Define quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Enable 4-bit quantization\n",
        "    llm_int8_enable_fp32_cpu_offload=True,  # Allow CPU offloading for FP32 modules\n",
        ")\n",
        "\n",
        "# Load the LoRA model and apply quantization\n",
        "if True:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"/content/lora_model\",  # Path to LoRA model\n",
        "        max_seq_length=128,\n",
        "        quantization_config=quantization_config,  # Apply quantization config\n",
        "        device_map=\"cuda\",  # Automatically distribute model across CPU and GPU\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)  # Enable faster inference\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"T·∫°i sao t√¥i n√™n d√πng m√°y gi·∫∑t?\",  # instruction\n",
        "            \"\",  # input\n",
        "            \"\",  # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}